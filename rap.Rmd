---
title: "RAP Guidance for Statistics Producers"
---

```{r include=FALSE}
require(knitr)
```

<p class="text-muted">Guidance for how to implement the principles of Reproducible Analytical Pipelines (RAP) into our production proccesses.</p>

---


# What is RAP


---

[Reproducible Analytical Pipelines](https://dataingovernment.blog.gov.uk/2017/03/27/reproducible-analytical-pipeline/){target="_blank"}, or RAP for short. The full words still hide the true meaning behind buzzwords and jargon though. What it actually means is using automation to our advantage when analysing data, and this is as simple as writing code such as a SQL query that we can click a button to execute and do the job for us. 


We already have 'analytical pipelines' and have done for many years. The aim of RAP, is to automate the parts of these pipelines that can be automated, to increase efficiency and accuracy, while creating a clear audit trail to allow analyses to easily be re-run if needed. This will free us up to focus on the parts of our work where our human input can really add value. RAP is something we can use to reduce the burden on us by getting rid of some of the boring stuff, what's not to like!


---


### Our scope

---

We want to focus on the parts of the production process that we have ownership and control over – so we are focussing on the process from data sources to publishable data files. This is the part of the process where RAP can currently add the most value - automating the production and quality assurance of our outputs currently takes up huge amount of analytical resource, which could be better spent providing insight and other value adding activity. 

`r knitr::include_graphics("images/RAP-Scope-Arrow.png")`

In Official Statistics production we are using RAP as a framework for best practice when producing our published data files, as these are the foundations of our publications moving forward. Following this framework will help us to improve and standardise our current production processes and provide a clear 'pipeline' for analysts to follow. This will have the added benefit of setting a clear and defined world of tools and skills required, making learning and development that much clearer and easier. To get started with RAP, we first need to be able to understand what it actually means in practice, and be able to [assess our own work against the principles of RAP](#what-we-need-to-do).

Implementing RAP for us will involve combining the use of SQL, R, and clear, consistent version control to increase efficiency and accuracy in our work. For more information on what these tools are, why we are using them, and resources to help upskill in thoses areas, see our [learning resources](l+d.html){target="_blank"} page.

The collection of, and routine checking of data as it is coming into the department is also an area that RAP can be applied to. We have kept this out of scope at the moment as the levels of control in this area vary wildly from team to team. If you would like advice and help to automate any particular processes, feel free to [contact us](mailto:explore.statistics@education.gov.uk).


---

# Core principles

---


1. **Data sources for a publication are stored in the same database** _- [Getting source data together](#getting-your-data-together-in-sql)_


2. **Underlying data files are produced using code, with no manual steps** _- [Using automation to process data](#using-code-to-process-data)_


3. **Files and scripts should be appropriately version controlled** _- [Using version control](#version-control)_


4. **There should be basic automated quality assurance applied to outputs** _- [Using automated QA](#automated-qa)_


---

# What we need to do

---

<div class="alert alert-dismissible alert-danger">
<strong>Teams are expected to review their own processes and use the guidance on this site</strong> to start making improvements towards meeting the four core principles if they aren't already. If you would like additional help to review your processes, please contact the [Statistics Development Team](mailto:explore.statistics@education.gov.uk).
</div>


Teams will start from different places and implement changes at different rates, and in different ways. We do not expect that every team will follow the same path, or even end at the same point. Don't worry if this seems overwhelming at first, use the guidance here to identify areas for improvement and then tackle them with confidence.

---


## Where we need to focus

---

Most teams have already made progress with their production of tidy data files, and the release of the automated screener has now tied up that end point of the pipeline that we are all currently working towards. The standard pipeline for all teams will roughly resemble this:


`r knitr::include_graphics("images/RAP-Process-Overview.png")`


The key now is for us to build on the work so far and focus on how we improve the quality and efficiency of our production processes up to that point. To do this, we need to make a concerted effort to standardise how we store and access our data, before then automating what we can to reduce the burden of getting the numbers ready and see the benefits of RAP. The exact meaning of this will vary within teams.


---


## RAP in practice

---

The diagram below highlights what RAP means for us, and the varying levels in which it can be applied. The general expectation is that all teams will have shifted towards the good practice implementation of RAP by the end of their next production cycle. It's worth acknowledging that some teams are already working around great and best practice levels.


`r knitr::include_graphics("images/RAP-in-practice.png")`

---

# Review checklists

---

Below are checklists designed to make reviewing our processes based on the graphic above easier, giving a straightforward list of quesitons to check our work against. This will flag areas that require improvement, and we can then use the links or sidebar to go to the specific section with more detail and guidance on how to get there.


Some teams will already be looking at best practice, while others will still have work to do to achieve good practice. We know that all teams are starting this from different points.

---

## Good practice

---

- Do we have a [sensible folder and file structure](#folder-structures)?


- Do we have suitable [documentation](#documentation)?


- Are our underlying data files [screened against the standards](#data-standards-screening)?


- Are all of our source data stored in a [single SQL database](#how-to-set-up-a-sql-working-area)?


- Do we produce our data with [no manual steps](#using-code-to-process-data)?


- Do we have a clean final version of the code used to produce the data?


- Do we use the [basic level of automated QA](#automated-qa)?

---

## Great practice

---

- Can the script that we used this year be used again next year with minimal alterations ( < 30 minutes of editing)?

- Do we have a [git-controlled repo](#how-to-setup-an-azure-devops-area) with final versions of code and documentation saved in it?

- Do we have the [data-screener setup](#data-standards-screening) and run within our team?

- Do we use a [single production script](#using-code-to-process-data) to get from source data to underlying data?

- Do we use publication-specific [automated QA](#automated-qa)?

- Do we use automation to produce [summary reports](#automating-summary-statistics) of our data?

---

## Best practice

---


- Do we actively use git when developing code?


- Do we have a single production script to get from source to underlying data, with intergrated QA and summary reports?


---

# Getting your data together in SQL

---

The first place to start for your teams RAP is to store the raw data you use to create underlying data in a Microsoft SQL Server database. This is similar to a sharepoint area or a shared folder, but it's a dedicated data storage area which allows multiple users to use the same file at once, and for you to run code against the data in one place.

---

### Introduction to SQL

---

SQL stands for Structured Query language and is a scripting language, which asks DfE servers to process data.


In order for us to be able to have an end-to-end data pipeline where we can replicate our analysis across the department, we should store all of the raw data needed to create aggregate statistics in a managed Microsoft SQL Server. This includes any lookup tables and all administrative data from collections prior to any manual processing. This allows us to then match and join the data together in an end-to-end process using SQL queries. 


SQL is a fantastic language for large scale data joining and manipulation; it allows us to replicate end-to-end from raw data to final aggregate statistics output. Having all the data in one place and processing it in one place makes our lives easier, and also helps us when auditing our work and ensuring reproducibility of results. 


For a collection of the best resources to use when leanrning SQL, see our [learning resources](l+d.html) page, and for guidance on best practice when writing SQL queries, see the [using code to process data](#using-code-to-process-data) and [documentation](#documentation) sections below. 

---

### How to set up a SQL working area

---

More to follow.

---

### How to get your data into SQL

---

More to follow.

---

#### Copying data from iStore

---

More to follow.

---

#### Importing data to SQL Server

---

There's lots of guidance online of how to import flat files from shared areas into Microsoft SQL server on the internet, including [this guide](https://docs.microsoft.com/en-us/sql/relational-databases/import-export/import-flat-file-wizard?view=sql-server-2017){target="_blank"}.


Remember that it is important to import them with consistent, throught-through [naming conventions](#naming-conventions). You will thank yourself later.

---

### How to grant access to your area

---

More to follow.

---

# Using code to process data

---

It is as simple as telling your computer what to do. Code is just a list of instructions in a language that your computer can understand. This brings numerous benefits, computers are far quicker, more accurate, and far more reliable than humans in many of the tasks that we do. Writing out these instructions saves us significant amounts of time, particularly when it can be reused in future years, or even next week when one specific number in the source file suddenly changes, and also provides us with auditable documentation for our production processes, saving the need for writing down information in extra documents. 

Reliability is a huge benefit of the automation that RAP brings - when one of the lines of data has to be amended a week before publication, it's a life saver to know that you can re-run your process in minutes, and ressuring to know that it will give you the result you want. You can run the same code 100 times, and be confident that it will follow the same steps in the same order every single time.

More guidance will follow on best practice in writing code, as well as useful snippets of code that everyone can make use of. For now, the key thing to remember is that **we should be automating everything we can**, avoiding any manual steps. This means that once teams have developed their code, they can then execute the code and have their underlying data files ready from a single click of a button. Designing our processes in this way will make them better documented, reduce human error, increasing effiency and reliability.

---

## Writing code

---

One great thing about code, and indeed coding standards, is that there are so many options to choose from. Well, that isn't always a great thing. It often leads to messy or inefficient code, and a lack of coherence in what we write. 

As such, we've collected together a few standards here to help you to follow best practice where possible and make your life easier when using SQL and R.

---

## SQL

---

For best pratice on writing SQL code, here is a particularly useful [word document](resources/TSQL_Coding_Standards.docx){target="_blank"} produced by our [Data Hub](https://educationgovuk.sharepoint.com/sites/DataHubProgramme2/Shared%20Documents/Content%20Management/Data%20Hub%20one%20pager.pdf){target="_blank"}. This outlines a variety of best practices, ranging from naming conventions, to  to formatting your SQL code so that it is easy to follow visually.

---

### Producing tidy underlying data in SQL

---

More guidance on this follow, for now, here is a [SQL query](https://github.com/TomFranklin/sql-applied-data-tidying/blob/master/data_tidying_l_and_d.sql){target="_blank"} that you can run on your own machine and walks you through the basics of tidying a simple example dataset in SQL.

---

### Connecting R to SQL

---

For now, take a look at the materials from Cathy Atkinson's coffee and coding session on [connecting R to SQL using DBI and odbc](https://educationgovuk.sharepoint.com/sites/sarpi/g/WorkplaceDocuments/Forms/AllItems.aspx?FolderCTID=0x012000C61C1076C17C5547A6D6D8C2A27B5D97&View=%7B2B35083D%2D7626%2D48E2%2D9615%2D451544742692%7D&id=%2Fsites%2Fsarpi%2Fg%2FWorkplaceDocuments%2FInducation%20learning%20and%20career%20development%2FCoffee%20and%20Coding%2F180718%5Fcathy%5FR%5FSQL%2FConnecting%5Fto%5FSQL%5FRevA%2Ehtml&parent=%2Fsites%2Fsarpi%2Fg%2FWorkplaceDocuments%2FInducation%20learning%20and%20career%20development%2FCoffee%20and%20Coding%2F180718%5Fcathy%5FR%5FSQL).

---

## R

---

When using R, it is generally best practice to use [R projects](https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects){target="_blank"} as directories for your work.

The recommended standard for styling your code in R, is the [tidyverse styling](https://style.tidyverse.org/){target="_blank"}, which is fast becoming the recognised standard. What is even better is that you can automate this using the [styler](https://styler.r-lib.org/){target="_blank"} package, which will literally style your code for you, and is well worth a look.

There is also plenty of guidance around the internet for [best practice](https://waterdata.usgs.gov/blog/intro-best-practices/){target="_blank"} when writing [efficient R code](https://waterdata.usgs.gov/blog/intro-best-practices/){target="_blank"}.

---

### Tidying and processing data in R

---


More will follow here, for now, [here is a video](https://vimeo.com/33727555){target="_blank"} of Hadley Wickham talking about how to tidy your data to these principles in R. This covers useful functions and how to complete common data tidying tasks in R. Also worth taking a look at [applied data tidying in R, by RStudio](https://www.youtube.com/watch?v=1ELALQlO-yM){target="_blank"}.


Using the `%>%` pipe in R can be incredibly powerful, and make your code much easier to follow, as well as more efficient. If you aren't yet familiar with this, have a look at [this article](https://seananderson.ca/2014/09/13/dplyr-intro/){target="_blank"} that provides a useful beginners guide to piping and the kinds of functions you can use it for. The possibilities stretch about as far as your imagination, and if you have a function or task you wanto do within a pipe, googling 'how do I do X in dplyr r' will usually start to point you in the right direction, alternatively you can [contact us](mailto:explore.statistics@education.gov.uk), and we'll be happy to help you figure out how to do what you need.

A quick example of how powerful this is is below, where my_data is proccesed to create new columns, have column names renamed, have the column names tidied using the [janitor](https://garthtarr.github.io/meatR/janitor.html){target="_blank"} package, blank rows and columns removed, data filtered to only include specific geographic levels, and rows rearranged in order, all in a few lines of easy to follow code:


```{r example, eval=FALSE}
processed_regional_data <- data %>% 
  mutate(newPercentageColumn = (numberColumn / totalPopulationColumn) * 100) %>% 
  rename(newPercentageColumn = percentageRate,
         numberColumn = number,
         totalPopulationColumn = population) %>% 
  clean_names() %>% 
  remove_empty() %>% 
  filter(geographic_level == "Regional") %>% 
  arrange(time_period, region_name)
```


For further resources on learning R so that you're able to apply it to your everyday work, have a look at the [learning resources](l+d.html){target="_blank"} page.

---


# Version control

---

The aim should be to leave your work in a state that others (including future you!), can pick it up and immediately find what they need, understanding the processes that have happened previously. Changes to files should be documented, and published versions should be clearly named and stored in their own folder. As we work with code to process our data more and more, we can begin to utilise version control software to make this process much easier, allowing simultaneous collaboration on files.

---

## Folder structures

---

How you organize and name your files will have a big impact on your ability to find those files later and to understand what they contain. You should be consistent and descriptive in naming and organizing files so that it is obvious where to find specific data and what the files contain.


As a minimum you should have a folder that includes all of the final versions of documents produced and published, per release, within a folder for the wider publication. Try asking yourself if it would be easy for someone who isn't in the team to find this folder, and if not, is there a better way that you could name and structure your folders to make them more intituitive to navigate?
 
---

## Naming conventions

---

Having a **clear** and **consistent** naming convention for your files is critical. Remember that file names should:


**Be machine readable** 

- Avoid spaces.
- Avoid special characters such as: ~ ! @ # $ % ^ & * ( ) ` ; < > ? , [ ] { } ‘ “.
- Be as short as practicable; overly long names do not work well with all types of software.


**Be human readable** 

- Be easy to understand the contents from the name.


**Play well with default ordering**

- Often (though not always!) you should have numbers first, particularly if your file names include dates.
- Follow the ISO 8601 date standard (YYYYMMDD) to ensure that all of your files stay in chronological order.
- Use leading zeros to left pad numbers and ensure files sort properly, avoiding 1,10,2,3.


If in doubt, take a look at this [presentation](https://speakerdeck.com/jennybc/how-to-name-files){target="_blank"}, or this [naming convention guide by Stanford](https://library.stanford.edu/research/data-management-services/data-best-practices/best-practices-file-naming){target="_blank"}, for examples reinforcing the above.

---

#### EES naming conventions

---

Use the principles above to decide on a name for each data file and your wider publication. Then apply those to the following conventions. Remember that the general public will be using these.


| File | Name |
|------|------|
| Data | data_file.csv |
| Metadata (EES) | data_file.meta.csv |
| Metadata (external guidance) | publication_name.year.metadata.pdf |
| PRA list | publication_name.year.pra-list.pdf |
| Methodology (if not in the platform as .html or link) | publication_name.methodology.pdf |


---

## Documentation

---

_When you assume you make an 'ass' out of 'u' and 'me'_. Everyone knows this saying, yet few of us heed it's warning. 


When documenting your processes you should leave nothing to chance, we all have wasted time in the past trying to work out what it was that we had done before, and that time increases even more when we are picking up someone elses work. Thorough documentation saves us time, and provides a clear audit trail of what we do. This is key for the 'Reproducible' part of RAP, our processes must be easily reproducible and clear documentation is fundamental to that.


You should be annotating as you go, ensuring that every process and decision made is written down. Take a look at your processes and be critical - could another analyst pick them up without you there to help them? If the answer is no (don't feel ashamed, it will be for many teams) then go through and note down areas that require improvement, so that you can revise them with your team. Imagine you have a new starter who understands the tools you've used but has never worked in your area before, and then document in a way that they could pick it up without assistance.

---

### Commenting in code

---

When writing code, whether that is SQL, R, or something else, make sure you're commenting as you go. Start off every file by outlining the date, author, purpose, and if applicable, the structure of the file, like this: 

```
----------------------------------------------------------------------------------------------
-- Script Name:		Section 251 Table A 2019 - s251_tA_2019.sql
-- Description:		Extraction of data from IStore and production of underlying data file
-- Author:		    Cam Race
-- Creation Date:   15/11/2019
----------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------
--//  Process
-- 1. Extract the data for each available year
-- 2. Match in extra geographical information
-- 3. Create aggregations - both categorical and geographical totals
-- 4. Tidy up and output results
-- 5. Metadata creation
----------------------------------------------------------------------------------------------
```

Commented lines should begin with -- (SQL) or # (R), followed by one space and your comment. Remember that **comments should explain the why, not the what.** Use commented lines of - to break up your files into scannable chunks based upon the structure and subheadings, like the one below, you'll thank yourself later:

```
# -----------------------------------------------------------------------------------
# 2 - Importing the data
```

You might be thinking that it would be nice if there was software that could help you with documentation, if so, read on, as Git is an incredibly powerful tool that can help us easily and thoroughly document versions of our files. If you're at the stage where you are developing your own functions and packages in R, then take a look at [roxygen2](https://roxygen2.r-lib.org/){target="_blank"} as well.


---

## Introduction to Git

---


More to follow, for now, take a look at at the [resources for learning Git](l+d.html#git){target="_blank"} on the learning resources page.


---

### How to setup an Azure DevOps area

---

More to follow.

---

### How to use Git in practice

---

More to follow, for now please refer to the resources above, or the other links on the [learning resources](l+d.html#git){target="_blank"} page.

---

# Automated QA

---

Any data files that have been created will need to be quality assured. These checks should be automated where possible, so the computer is doing the hard work - saving us time, and to ensure their reliability.


Some teams are already making great progress with automated QA and realising the benefits of it. The Statistics Development Team are working with these to provide generalised code that teams can use as a starting point for automated QA. The intention is that teams can then run this as a minimum, before then looking to develop more area specific checks to the script and/or continue with current checking processes in tandem. If your team already use, or are working towards using, automated QA then get in touch as we'd be keen to see what you have.


It is assumed that when using R, automated scripts will output .html reports that the team can read through to understand their data and identify any issues, and save as a part of their process documentation.

---

### Recommended generic checks

---

More will follow here as we work with teams to develop generalisable automated scripts for QA. For those who are interested in starting writing their own QA scripts, it's worth looking at packages in R such as [testthat](https://testthat.r-lib.org/){target="_blank"}, including the [coffee and coding talk](https://educationgovuk.sharepoint.com/sites/sarpi/g/WorkplaceDocuments/Forms/AllItems.aspx?RootFolder=/sites/sarpi/g/WorkplaceDocuments/Inducation%20learning%20and%20career%20development/Coffee%20and%20Coding/190306_peter_autotesting&FolderCTID=0x012000C61C1076C17C5547A6D6D8C2A27B5D97&View=%7b2B35083D-7626-48E2-9615-451544742692%7d){target="_blank"} on it by Peter Curtis, as well as this [guide on testing](http://r-pkgs.had.co.nz/tests.html){target="_blank"} by Hadley Wickham. 


The [janitor](https://garthtarr.github.io/meatR/janitor.html){target="_blank"} package in R also has some particularly useful functions, such as `clean_names()` to automatically clean up your variable names, `remove_empty()` to remove any completely empty rows and columns, and `get_dupes()` which retrieves any duplicate rows in your data - this last one is particularly powerful as you can feed it specific columns and see if there's any duplicate instances of values across those columns.

---

### Examples of this in practice

---

More to follow.

---


### Automating summary statistics

---

As a part of automating QA, we should also be looking to automate the production of summary statistics alongside the tidy underlying data files, this then provides us with instant insight into the stories underneath the numbers. More guidance, including generalisable code, will be added here in the future.

---

# Data standards screening

---

The final part of quality assurance for underlying data files, will be to check that they meet the standards that have been agreed upon. Once teams have quality assured the data files, they can then run them through the data screener to check they meet the standards required for the EES platform. This forms the final part of the automated QA process. 


The screener outputs a .html report that we can read through to make sure all checks have passed. If any checks fail, teams should make the required changes to the data (potentially running full QA again) and then re-run the screener. If all the screener checks pass, and only if they all pass and the report shows 100%, the file is then ready to be loaded into the EES platform. 


The data-screener can found on [GitHub](https://github.com/dfe-analytical-services/ees-data-screener){target="_blank"}, and from there can be either downloaded or cloned. A [tutorial video](https://www.youtube.com/watch?v=D60UU5r_TrM&feature=youtu.be){target="_blank"} has been made to walk you through the initial set-up and use of the project.


---